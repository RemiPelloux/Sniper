"""
Generic Vulnerability Scanner Integration

This module provides a comprehensive vulnerability scanner that can be used to test for
common web application vulnerabilities using the payload library.
"""

import asyncio
import json
import logging
import os
import re
import time
import urllib.parse
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union
from urllib.parse import parse_qs, urlencode, urljoin, urlparse

import requests
from pydantic import BaseModel
from requests.exceptions import RequestException

from src.integrations.base import ToolIntegration, ToolIntegrationError
from src.integrations.url_filter import UrlFilter
from src.payloads import get_all_payloads, load_payloads
from src.results.types import BaseFinding, FindingSeverity, WebFinding

# Configure logger
log = logging.getLogger(__name__)

# Default user agent for requests
DEFAULT_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# Vulnerability types and their detection patterns
VULN_TYPES = {
    "sqli": {
        "name": "SQL Injection",
        "description": "Vulnerability allowing an attacker to inject SQL commands",
        "severity": FindingSeverity.HIGH,
        "detection_patterns": [
            "sql syntax",
            "mysql",
            "ORA-",
            "sqlite",
            "PostgreSQL",
            "SQLSTATE",
            "Warning: mysql",
            "you have an error in your sql syntax",
            "unclosed quotation",
            "quoted string",
        ],
    },
    "xss": {
        "name": "Cross-Site Scripting (XSS)",
        "description": "Vulnerability allowing injection of malicious scripts into web pages",
        "severity": FindingSeverity.MEDIUM,
        "detection_patterns": [
            "<script>",
            "alert(",
            "onerror=",
            "onload=",
            "javascript:",
        ],
    },
    "open_redirect": {
        "name": "Open Redirect",
        "description": "Vulnerability allowing redirection to arbitrary external domains",
        "severity": FindingSeverity.MEDIUM,
        "detection_patterns": [
            "redirect=",
            "url=",
            "return_to=",
            "redir=",
            "return_url=",
        ],
    },
    "path_traversal": {
        "name": "Path Traversal",
        "description": "Vulnerability allowing access to files outside the web root directory",
        "severity": FindingSeverity.HIGH,
        "detection_patterns": [
            "../",
            "..\\",
            "%2e%2e%2f",
            "file://",
            "/etc/passwd",
            "\\Windows\\",
        ],
    },
    "command_injection": {
        "name": "Command Injection",
        "description": "Vulnerability allowing execution of arbitrary commands on the host",
        "severity": FindingSeverity.CRITICAL,
        "detection_patterns": [
            "uid=",
            "gid=",
            "groups=",
            "Windows NT Version",
            "Directory of ",
            "Directory listing",
            "Volume Serial Number",
            "root:x:",
            "sbin",
        ],
    },
    "ssrf": {
        "name": "Server-Side Request Forgery",
        "description": "Vulnerability allowing the server to make requests to internal resources",
        "severity": FindingSeverity.HIGH,
        "detection_patterns": [
            "AWS_SECRET_ACCESS_KEY",
            "AKIA",
            "X-Amz-Security-Token",
            "metadata",
            "169.254.169.254",
            "localhost",
            "127.0.0.1",
        ],
    },
    "xxe": {
        "name": "XML External Entity Injection",
        "description": "Vulnerability allowing reading of local files via XML processing",
        "severity": FindingSeverity.HIGH,
        "detection_patterns": [
            "root:x:",
            "/bin/bash",
            "daemon:",
            "file not found",
            "[SYSTEM",
            "WINDOWS",
            "boot loader",
            "at line",
        ],
    },
    "file_inclusion": {
        "name": "File Inclusion",
        "description": "File inclusion vulnerability that allows attackers to include local or remote files.",
        "severity": "HIGH",
        "detection_patterns": [
            "failed to open stream",
            "include_path",
            "failed opening",
            "no such file",
            "failed to read",
            "cannot include",
            "</html><!DOCTYPE html>",  # Pattern for when a file is included twice
        ],
    },
}


class VulnerabilityFinding(BaseModel):
    """Represents a discovered vulnerability finding"""

    title: str
    description: str
    severity: str
    url: str
    vulnerability_type: str
    payload: Optional[str] = None
    evidence: Optional[str] = None
    request_method: Optional[str] = None
    timestamp: Optional[str] = None

    def to_web_finding(self) -> WebFinding:
        """Convert to a WebFinding object for result normalization"""
        severity_map = {
            "CRITICAL": FindingSeverity.CRITICAL,
            "HIGH": FindingSeverity.HIGH,
            "MEDIUM": FindingSeverity.MEDIUM,
            "LOW": FindingSeverity.LOW,
            "INFO": FindingSeverity.INFO,
        }

        # Map string severity to FindingSeverity enum
        severity_enum = severity_map.get(self.severity.upper(), FindingSeverity.MEDIUM)

        return WebFinding(
            title=self.title,
            description=self.description,
            severity=severity_enum,
            url=self.url,
            evidence=self.evidence or "",
            request_method=self.request_method or "GET",
            tool="vulnerability_scanner",
            raw_output=json.dumps(self.dict()),
            target=self.url,
            source_tool="vulnerability_scanner",
        )


class VulnerabilityScanner(ToolIntegration):
    """Integration for generic vulnerability scanning"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": DEFAULT_USER_AGENT,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.5",
                "Accept-Encoding": "gzip, deflate",
                "Connection": "close",
            }
        )
        self.findings: List[VulnerabilityFinding] = []
        self.crawled_urls: Set[str] = set()
        self.urls_to_scan: List[str] = []
        # Add tracking for vulnerable parameters to avoid duplicate testing
        self.vulnerable_parameters: Dict[str, Set[str]] = (
            {}
        )  # {url+param: {vuln_types}}

    @property
    def tool_name(self) -> str:
        return "vulnerability_scanner"

    def check_prerequisites(self) -> bool:
        """Check if all prerequisites are met for using this scanner"""
        # This scanner only requires Python standard library and requests
        return True

    async def run(
        self, target: str, options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Run a vulnerability scan against the target

        Args:
            target: The URL, domain, or IP to scan
            options: Additional options for the scan

        Returns:
            Dictionary with scan results

        Raises:
            ToolIntegrationError: If the scan fails
        """
        try:
            # Default options
            verify_ssl = True
            max_urls = 30
            scan_types = list(VULN_TYPES.keys())
            scan_depth = "quick"  # Alternatives: quick, standard, comprehensive
            timeout = 10
            initial_urls = []
            wait_time = 0

            # Override defaults with provided options
            if options:
                verify_ssl = options.get("verify_ssl", verify_ssl)
                max_urls = options.get("max_urls", max_urls)
                scan_types = options.get("scan_types", scan_types)
                scan_depth = options.get("scan_depth", scan_depth)
                timeout = options.get("timeout", timeout)
                initial_urls = options.get("initial_urls", initial_urls)
                wait_time = options.get("wait_time", wait_time)

            log.info(f"Starting vulnerability scan against {target}")
            log.info(f"Scan types: {', '.join(scan_types)}")
            log.info(f"Scan depth: {scan_depth}")

            # Make sure target is a URL
            target = self._normalize_url(target)

            # Clear previous run data
            self.findings = []
            self.crawled_urls = set()
            self.urls_to_scan = [target]
            self.vulnerable_parameters = {}

            # Add initial URLs if provided
            if initial_urls:
                for url in initial_urls:
                    if url not in self.urls_to_scan:
                        self.urls_to_scan.append(url)

            # Adjust session settings
            self.session.verify = verify_ssl

            # Step 1: Crawl the target to discover endpoints
            await self._crawl(max_urls, timeout, wait_time)

            # Step 2: Scan each discovered endpoint for vulnerabilities
            await self._scan_endpoints(scan_types, scan_depth, timeout)

            # Convert findings to result format
            result = {
                "target": target,
                "scan_time": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime()),
                "scan_types": scan_types,
                "scan_depth": scan_depth,
                "urls_crawled": len(self.crawled_urls),
                "findings": [finding.dict() for finding in self.findings],
            }

            log.info(
                f"Vulnerability scan completed. Found {len(self.findings)} issues."
            )
            return result

        except Exception as e:
            log.exception(f"Error during vulnerability scan: {str(e)}")
            raise ToolIntegrationError(f"Vulnerability scan failed: {str(e)}")

    def parse_output(self, raw_output: Dict[str, Any]) -> Optional[List[BaseFinding]]:
        """Parse the scan output into normalized findings

        Args:
            raw_output: The raw output from the run method

        Returns:
            List of BaseFinding objects
        """
        findings = []

        if "findings" in raw_output:
            for finding_dict in raw_output["findings"]:
                try:
                    # Convert raw finding dict to VulnerabilityFinding
                    finding = VulnerabilityFinding(**finding_dict)
                    # Convert to WebFinding for normalization
                    web_finding = finding.to_web_finding()
                    findings.append(web_finding)
                except Exception as e:
                    log.error(f"Error parsing finding: {str(e)}")

        return findings

    async def _crawl(self, max_urls: int, timeout: int, wait_time: int = 0) -> None:
        """Crawl the target to discover endpoints

        Args:
            max_urls: Maximum number of URLs to crawl
            timeout: Request timeout in seconds
            wait_time: Wait time between requests in seconds
        """
        log.info("Crawling website to discover endpoints...")

        # Keep track of when we started for optimization
        start_time = time.time()

        # Initialize URL filter with the base URL
        url_filter = UrlFilter()
        first_url = self.urls_to_scan[0] if self.urls_to_scan else None
        if first_url:
            url_filter.set_base_url(first_url)

        # OPTIMIZATION: Limit the crawl time to prevent excessive crawling
        max_crawl_time = 60  # Maximum crawl time in seconds

        # Pre-normalize all URLs in the queue
        normalized_urls = []
        for url in self.urls_to_scan:
            normalized_url = url_filter.normalize_url(url)
            if normalized_url not in normalized_urls:
                normalized_urls.append(normalized_url)

        # Replace the original URLs with normalized ones
        self.urls_to_scan = normalized_urls

        # If we have multiple URLs, prioritize them based on value
        if len(self.urls_to_scan) > 1:
            self.urls_to_scan = url_filter.prioritize_urls(self.urls_to_scan)

        while self.urls_to_scan and len(self.crawled_urls) < max_urls:
            # Check if we've exceeded the maximum crawl time
            if time.time() - start_time > max_crawl_time:
                log.info(
                    f"Maximum crawl time of {max_crawl_time}s reached. Stopping crawl."
                )
                break

            current_url = self.urls_to_scan.pop(0)

            # Skip if already crawled
            if current_url in self.crawled_urls:
                continue

            try:
                log.debug(f"Crawling: {current_url}")
                response = self.session.get(
                    current_url, timeout=timeout, allow_redirects=True
                )
                self.crawled_urls.add(current_url)

                # Extract forms for testing
                if "<form" in response.text:
                    forms = self._extract_forms(response.text)
                    for form_action in forms:
                        form_url = urllib.parse.urljoin(current_url, form_action)
                        # Normalize the form URL
                        form_url = url_filter.normalize_url(form_url, current_url)
                        if (
                            form_url not in self.crawled_urls
                            and form_url not in self.urls_to_scan
                        ):
                            # Check if the URL is allowed by the filter
                            if url_filter.is_allowed(form_url, current_url):
                                # Prioritize form URLs as they're high-value targets
                                self.urls_to_scan.insert(0, form_url)

                # Extract additional URLs
                new_urls = self._extract_urls(response.text, current_url)

                # Filter, normalize and prioritize the new URLs
                filtered_urls = []
                for url in new_urls:
                    # Skip URLs we've already seen
                    if (
                        url in self.crawled_urls
                        or url in self.urls_to_scan
                        or url in filtered_urls
                    ):
                        continue

                    # Normalize the URL
                    normalized_url = url_filter.normalize_url(url, current_url)

                    # Check if the URL is allowed by the filter
                    if url_filter.is_allowed(normalized_url, current_url):
                        filtered_urls.append(normalized_url)

                # Prioritize the filtered URLs
                if filtered_urls:
                    prioritized_urls = url_filter.prioritize_urls(filtered_urls)

                    # Add the new URLs to the queue
                    for url in prioritized_urls:
                        self.urls_to_scan.append(url)

            except Exception as e:
                log.warning(f"Error crawling {current_url}: {str(e)}")

            # Wait between requests
            if wait_time > 0:
                await asyncio.sleep(wait_time)

        total_crawl_time = time.time() - start_time
        log.info(
            f"Crawled {len(self.crawled_urls)} URLs in {total_crawl_time:.2f} seconds"
        )

        # Sort the remaining URLs in the queue by priority
        if self.urls_to_scan:
            self.urls_to_scan = url_filter.prioritize_urls(self.urls_to_scan)

    async def _scan_endpoints(
        self, scan_types: List[str], scan_depth: str, timeout: int
    ) -> None:
        """Scan endpoints for vulnerabilities

        Args:
            scan_types: List of vulnerability types to scan for
            scan_depth: Scan depth (quick, standard, comprehensive)
            timeout: Request timeout in seconds
        """
        log.info("Testing endpoints for vulnerabilities...")

        # Determine how many payloads to use based on scan depth
        payloads_limit = {
            "quick": 3,
            "standard": 10,
            "comprehensive": 15,  # Limit to a reasonable number even in comprehensive mode
        }.get(scan_depth.lower(), 5)

        # Load payloads for each vulnerability type
        all_payloads = {}
        for vuln_type in scan_types:
            try:
                payloads = load_payloads(vuln_type)
                if payloads_limit is not None:
                    payloads = payloads[:payloads_limit]
                all_payloads[vuln_type] = payloads
            except (FileNotFoundError, ValueError):
                log.warning(f"No payloads found for vulnerability type: {vuln_type}")
                all_payloads[vuln_type] = []

        # OPTIMIZATION: Sort URLs by likelihood of vulnerability
        # URLs with parameters, forms, or key patterns are higher priority
        high_value_urls = []
        medium_value_urls = []
        low_value_urls = []

        value_patterns = {
            "high": [
                r"\?",
                r"login",
                r"search",
                r"admin",
                r"upload",
                r"file",
                r"register",
                r"contact",
            ],
            "medium": [
                r"profile",
                r"user",
                r"account",
                r"product",
                r"item",
                r"view",
                r"display",
            ],
        }

        for url in self.crawled_urls:
            # URLs with query parameters are high value
            parsed_url = urlparse(url)
            if parsed_url.query:
                high_value_urls.append(url)
            # Check for high-value patterns
            elif any(
                re.search(pattern, url, re.IGNORECASE)
                for pattern in value_patterns["high"]
            ):
                high_value_urls.append(url)
            # Check for medium-value patterns
            elif any(
                re.search(pattern, url, re.IGNORECASE)
                for pattern in value_patterns["medium"]
            ):
                medium_value_urls.append(url)
            else:
                low_value_urls.append(url)

        # OPTIMIZATION: Limit the number of URLs to test
        # In a large site, we don't need to test all URLs - focus on high and medium value
        max_urls_to_test = min(len(self.crawled_urls), 50)  # Test at most 50 URLs

        # Prioritize high and medium value URLs
        urls_to_test = high_value_urls
        if len(urls_to_test) < max_urls_to_test:
            urls_to_test.extend(
                medium_value_urls[: max_urls_to_test - len(urls_to_test)]
            )
        if len(urls_to_test) < max_urls_to_test:
            urls_to_test.extend(low_value_urls[: max_urls_to_test - len(urls_to_test)])

        log.info(
            f"Testing {len(urls_to_test)} URLs prioritized by value (out of {len(self.crawled_urls)} discovered)"
        )

        # Further optimization: use async/await for concurrent testing
        async def test_url(url):
            # Parse URL to get query parameters
            parsed_url = urlparse(url)
            query_params = parse_qs(parsed_url.query)

            # If URL has query parameters, test each parameter for vulnerabilities
            if query_params:
                for param_name, param_values in query_params.items():
                    original_value = param_values[0] if param_values else ""
                    await self._test_parameter(
                        url,
                        param_name,
                        original_value,
                        scan_types,
                        all_payloads,
                        "GET",
                        timeout,
                    )

            # For POST endpoints, try testing with various payloads
            if self._is_likely_form_endpoint(url):
                await self._test_post_endpoint(url, scan_types, all_payloads, timeout)

        # Run tests concurrently with a limit of 5 concurrent tests to avoid overwhelming the server
        tasks = []
        for url in urls_to_test:
            tasks.append(test_url(url))

            # Process in batches of 5 to avoid overwhelming the server
            if len(tasks) >= 5:
                await asyncio.gather(*tasks)
                tasks = []

        # Process any remaining tasks
        if tasks:
            await asyncio.gather(*tasks)

    def _is_likely_form_endpoint(self, url: str) -> bool:
        """Check if URL is likely to be a form endpoint"""
        # Check for common form endpoint patterns
        patterns = [
            r"/login",
            r"/register",
            r"/signup",
            r"/search",
            r"/contact",
            r"/form",
            r"/submit",
            r"/process",
            r"/api",
            r"/action",
            r"/upload",
        ]

        for pattern in patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return True

        return False

    async def _test_parameter(
        self,
        url: str,
        param_name: str,
        original_value: str,
        scan_types: List[str],
        all_payloads: Dict[str, List[str]],
        method: str = "GET",
        timeout: int = 10,
    ) -> None:
        """Test a specific parameter for vulnerabilities

        Args:
            url: The URL to test
            param_name: The parameter name
            original_value: The original parameter value
            scan_types: List of vulnerability types to test
            all_payloads: Dictionary of payloads to use
            method: HTTP method (GET or POST)
            timeout: Request timeout in seconds
        """
        # Create a unique key for this parameter
        param_key = f"{url}:{method}:{param_name}"

        # If we don't have an entry for this parameter yet, create one
        if param_key not in self.vulnerable_parameters:
            self.vulnerable_parameters[param_key] = set()

        # Test each vulnerability type
        for vuln_type in scan_types:
            # Skip if we already found this vulnerability type for this parameter
            if vuln_type in self.vulnerable_parameters[param_key]:
                log.debug(
                    f"Skipping {vuln_type} testing for {url} - {param_name} (already vulnerable)"
                )
                continue

            if vuln_type not in all_payloads:
                continue

            for payload in all_payloads[vuln_type]:
                try:
                    # Send the request with the modified parameter
                    if method == "GET":
                        modified_url = self._modify_parameter(url, param_name, payload)
                        response = self.session.get(
                            modified_url, timeout=timeout, allow_redirects=False
                        )
                    else:  # POST
                        data = {param_name: payload}
                        response = self.session.post(
                            url, data=data, timeout=timeout, allow_redirects=False
                        )

                    # Check for vulnerability detection
                    detected = self._check_vulnerability(response, vuln_type, payload)
                    if detected:
                        self._add_finding(
                            url,
                            param_name,
                            vuln_type,
                            payload,
                            self._extract_evidence(response, vuln_type),
                            method,
                        )
                        log.info(
                            f"Found {vuln_type} vulnerability in {url} - parameter: {param_name}"
                        )

                        # Mark this parameter as vulnerable to this type
                        self.vulnerable_parameters[param_key].add(vuln_type)

                        # Skip remaining payloads for this vulnerability type
                        break

                except Exception as e:
                    log.debug(f"Error testing {url} for {vuln_type}: {str(e)}")

    async def _test_post_endpoint(
        self,
        url: str,
        scan_types: List[str],
        all_payloads: Dict[str, List[str]],
        timeout: int = 10,
    ) -> None:
        """Test a POST endpoint with various payloads

        Args:
            url: The URL to test
            scan_types: List of vulnerability types to test
            all_payloads: Dictionary of payloads to use
            timeout: Request timeout in seconds
        """
        # Common parameter names to test based on URL patterns
        common_params = self._get_common_params_for_url(url)

        # Test each parameter with each payload
        for param in common_params:
            # Create a unique key for this parameter
            param_key = f"{url}:POST:{param}"

            # If we don't have an entry for this parameter yet, create one
            if param_key not in self.vulnerable_parameters:
                self.vulnerable_parameters[param_key] = set()

            for vuln_type in scan_types:
                # Skip if we already found this vulnerability type for this parameter
                if vuln_type in self.vulnerable_parameters[param_key]:
                    log.debug(
                        f"Skipping {vuln_type} testing for {url} (POST) - {param} (already vulnerable)"
                    )
                    continue

                if vuln_type not in all_payloads:
                    continue

                for payload in all_payloads[vuln_type]:
                    try:
                        # Create POST data with payload
                        data = {param: payload}

                        # Add some default values for other common parameters
                        if param != "email" and "email" in common_params:
                            data["email"] = "test@example.com"
                        if param != "password" and "password" in common_params:
                            data["password"] = "Test123!"
                        if param != "username" and "username" in common_params:
                            data["username"] = "testuser"

                        response = self.session.post(
                            url, data=data, timeout=timeout, allow_redirects=False
                        )

                        # Check for vulnerability detection
                        detected = self._check_vulnerability(
                            response, vuln_type, payload
                        )
                        if detected:
                            self._add_finding(
                                url,
                                param,
                                vuln_type,
                                payload,
                                self._extract_evidence(response, vuln_type),
                                "POST",
                            )
                            log.info(
                                f"Found {vuln_type} vulnerability in {url} (POST) - parameter: {param}"
                            )

                            # Mark this parameter as vulnerable to this type
                            self.vulnerable_parameters[param_key].add(vuln_type)

                            # Skip remaining payloads for this vulnerability type
                            break

                    except Exception as e:
                        log.debug(
                            f"Error testing {url} (POST) for {vuln_type} in parameter {param}: {str(e)}"
                        )

    def _get_common_params_for_url(self, url: str) -> List[str]:
        """Get common parameter names to test based on URL patterns"""
        common_params = [
            "username",
            "password",
            "email",
            "query",
            "q",
            "id",
            "search",
            "name",
        ]

        # Add endpoint-specific parameters
        if re.search(r"/login", url, re.IGNORECASE):
            common_params.extend(["user", "uname", "user_id", "account"])
        elif re.search(r"/register", url, re.IGNORECASE):
            common_params.extend(
                [
                    "confirm_password",
                    "passwordRepeat",
                    "user",
                    "first_name",
                    "last_name",
                ]
            )
        elif re.search(r"/search", url, re.IGNORECASE):
            common_params.extend(["filter", "sort", "category", "tag", "keyword"])
        elif re.search(r"/api/", url, re.IGNORECASE):
            common_params.extend(["token", "api_key", "key", "action", "method"])

        return common_params

    def _modify_parameter(self, url: str, param_name: str, payload: str) -> str:
        """Modify a URL parameter with a payload"""
        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)

        # Modify the specific parameter
        query_params[param_name] = [payload]

        # Rebuild the query string
        new_query = urlencode(query_params, doseq=True)

        # Reconstruct the URL
        new_url = parsed_url._replace(query=new_query).geturl()

        return new_url

    def _check_vulnerability(
        self, response: requests.Response, vuln_type: str, payload: str
    ) -> bool:
        """Check if a vulnerability is detected in the response"""
        # Get vulnerability detection patterns
        vuln_info = VULN_TYPES.get(vuln_type, {})
        patterns = vuln_info.get("detection_patterns", [])

        # Check response status code for open redirect
        if vuln_type == "open_redirect" and 300 <= response.status_code < 400:
            redirect_url = response.headers.get("Location", "")
            if any(
                pattern in redirect_url.lower()
                for pattern in ["evil.com", "javascript:"]
            ):
                return True

        # Check if any patterns are found in the response
        response_text = response.text.lower()
        response_headers = str(response.headers).lower()

        for pattern in patterns:
            if pattern.lower() in response_text or pattern.lower() in response_headers:
                return True

        # Additional checks for specific vulnerability types
        if vuln_type == "sqli":
            sql_error_patterns = [
                "sql syntax",
                "mysql error",
                "odbc",
                "unclosed quotation",
                "quoted string",
                "sql error",
            ]
            if any(pattern in response_text for pattern in sql_error_patterns):
                return True

        elif vuln_type == "xss":
            # Check if the payload is reflected in the response
            if payload.lower() in response_text:
                return True

        return False

    def _extract_evidence(self, response: requests.Response, vuln_type: str) -> str:
        """Extract evidence of vulnerability from the response"""
        response_text = response.text.lower()

        # Get vulnerability detection patterns
        vuln_info = VULN_TYPES.get(vuln_type, {})
        patterns = vuln_info.get("detection_patterns", [])

        for pattern in patterns:
            pattern_index = response_text.find(pattern.lower())
            if pattern_index != -1:
                # Extract some context around the pattern (up to 100 characters)
                start = max(0, pattern_index - 50)
                end = min(len(response_text), pattern_index + len(pattern) + 50)
                return response_text[start:end]

        # Default evidence
        return (
            f"Status: {response.status_code}, Content snippet: {response_text[:200]}..."
        )

    def _add_finding(
        self,
        url: str,
        param_name: str,
        vuln_type: str,
        payload: str,
        evidence: str,
        method: str,
    ) -> None:
        """Add a vulnerability finding"""
        vuln_info = VULN_TYPES.get(vuln_type, {})
        title = f"{vuln_info.get('name', vuln_type.upper())} in {param_name}"
        description = f"{vuln_info.get('description', '')} in parameter '{param_name}'"
        severity = vuln_info.get("severity", FindingSeverity.MEDIUM)

        finding = VulnerabilityFinding(
            title=title,
            description=description,
            severity=str(severity).split(".")[-1],  # Extract name from enum
            url=url,
            vulnerability_type=vuln_type,
            payload=payload,
            evidence=evidence,
            request_method=method,
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime()),
        )

        self.findings.append(finding)

    def _extract_forms(self, html: str) -> List[str]:
        """Extract form actions from HTML"""
        # Simple regex-based extraction
        form_actions = []
        action_regex = re.compile(r'<form[^>]*action=["\'](.*?)["\']', re.IGNORECASE)
        matches = action_regex.findall(html)
        form_actions.extend(matches)

        return form_actions

    def _extract_urls(self, html: str, base_url: str) -> List[str]:
        """Extract URLs from HTML"""
        extracted_urls = set()

        # Extract href attributes
        href_regex = re.compile(r'href=["\'](.*?)["\']', re.IGNORECASE)
        matches = href_regex.findall(html)

        for href in matches:
            if href and not href.startswith("#") and not href.startswith("javascript:"):
                try:
                    full_url = urllib.parse.urljoin(base_url, href)
                    # Only include URLs from the same host
                    if urlparse(full_url).netloc == urlparse(base_url).netloc:
                        extracted_urls.add(full_url)
                except Exception as e:
                    log.debug(f"Error processing URL {href}: {str(e)}")

        # Extract API endpoints from JavaScript
        api_patterns = ["/api/", "/rest/", "/v1/", "/v2/", "/v3/"]
        for pattern in api_patterns:
            try:
                pattern_regex = re.compile(f"{pattern}([a-zA-Z0-9_/-]+)", re.IGNORECASE)
                matches = pattern_regex.findall(html)

                for match in matches:
                    api_url = f"{pattern}{match}"
                    full_url = urllib.parse.urljoin(base_url, api_url)
                    extracted_urls.add(full_url)
            except Exception as e:
                log.debug(f"Error extracting API endpoints: {str(e)}")

        # Look for URLs in JavaScript - common patterns
        js_patterns = [
            r'url:\s*[\'"]([^\'")]+)[\'"]',
            r'href:\s*[\'"]([^\'")]+)[\'"]',
            r'\.get\([\'"]([^\'")]+)[\'"]',
            r'\.post\([\'"]([^\'")]+)[\'"]',
            r'\.ajax\([{\s\S]*?url:\s*[\'"]([^\'")]+)[\'"]',
            r'fetch\([\'"]([^\'")]+)[\'"]',
            r'window\.location\s*=\s*[\'"]([^\'")]+)[\'"]',
        ]

        for pattern in js_patterns:
            try:
                url_regex = re.compile(pattern, re.IGNORECASE)
                matches = url_regex.findall(html)

                for match in matches:
                    if (
                        match
                        and not match.startswith("#")
                        and not match.startswith("javascript:")
                    ):
                        full_url = urllib.parse.urljoin(base_url, match)
                        # Check if the domain matches
                        if urlparse(full_url).netloc == urlparse(base_url).netloc:
                            extracted_urls.add(full_url)
            except Exception as e:
                log.debug(f"Error extracting JS URLs: {str(e)}")

        return list(extracted_urls)

    def _normalize_url(self, url: str) -> str:
        """Normalize URL to ensure it has a protocol"""
        if not url:
            raise ValueError("URL cannot be empty")

        # If no protocol specified, assume HTTP
        if not url.startswith(("http://", "https://")):
            url = f"http://{url}"

        return url
